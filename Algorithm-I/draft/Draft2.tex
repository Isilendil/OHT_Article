
\documentclass[twocolumn]{article}

\usepackage{graphicx}
\usepackage{subfigure}

\begin{document}


\section{Experimental Results}

In this section, we empirically evaluate the performance of proposed online heterogeneous transfer learning algorithms and classic online Passive-Aggressive algorithms (PA).
Encouraging results demonstrate that the proposed algorithms outperform these algorithms.

% dataset & task description
\subsection{Dataset}
Our experiments are conducted for image classification by leveraging information from text data.
We use NUS-WIDE dataset to generate learning tasks.
The NUS-WIDE dataset is extracted from Flickr.
It includes 269,648 images and the associated tags from Flickr, with a total number of 5,018 unique tags.
An image instance is represented by a feature vector based on SIFT descriptions, and a text instance is represented by a feature vector based on tags.
There are 81 ground-truth class labels in the dataset.
We randomly selected 10 classes (bird, boat, car, flower, food, rock, sun, toy, tree) and built $C_{10}^{2} = 45$ binary classification tasks.

We refer the images as data in the target domain, and the tags as the text data in the heterogeneous source domain.
Each binary classification task has 500 image instances in the target domain, 1,200 text instances in the heterogeneous source domain, and 1,500 co-occurred image-text pairs.
In order to obtain stable results, we draw 100 times of random permutation of the image instances in the target domain and evaluate the performance of learning algorithms based on mean and standard deviation of mistake rates.

% methods to be compared
% experimental setup 
%   kernel function
%   parameters setting
%   parameter sensitivity
%   times
%   evaluation metric

\subsection{Baseline Methods}
We compare the proposed methods with Passive-Aggressive (PA) online learning algorithms.
PA algorithm proposed by Crammer et al. does not exploit knowledge from the source domain.
It deals with the traditional online learning problem in the target domain.

For fair comparison and simplicity, we adopt Gaussian kernel function in all the algorithms and tasks.
The kernel parameter $\sigma = 8$ for the target domain.
The regularization parameter $C = 5$, $ \beta = \frac{\sqrt{T}}{\sqrt{T}+\sqrt{2\ln{4}}} $ for OHT-II algorithm.
In addition, we set the number of nearest neighbors to be considered $K = 10$.
Sensitivity of parameters will be examined in subsequent sections.

\subsection{Results and Discussion}

% figures of mean mistake rates
% tables of mean and standard derivation of mistake rates
% significant test results
% Cohen's d

\begin{table}[!htb]
\tiny
\caption{Results of all 45 tasks}
\begin{tabular}{|c||c|c|c|}
\hline\hline
Task &	PA                            & OHT1                          & OHT2 \\
\hline\hline
%\input{err_table_tex}
1 & 46.9680 $\pm$ 2.0210 & 33.5300 $\pm$ 0.3860 & \textbf{33.4120 $\pm$ 0.2709}\\
2 & \textbf{36.3640 $\pm$ 1.7050} & 37.6100 $\pm$ 1.5417 & 37.3700 $\pm$ 1.8823\\
3 & 44.7760 $\pm$ 1.8058 & \textbf{37.0000 $\pm$ 0.4989} & 37.5260 $\pm$ 0.4165\\
4 & 48.6120 $\pm$ 2.1255 & 30.0240 $\pm$ 0.3207 & \textbf{29.9780 $\pm$ 0.2665}\\
5 & 41.0060 $\pm$ 1.8923 & 24.5420 $\pm$ 0.2818 & \textbf{24.4820 $\pm$ 0.1452}\\
6 & 39.5800 $\pm$ 1.8349 & \textbf{24.2720 $\pm$ 0.2336} & 24.3000 $\pm$ 0.1407\\
7 & \textbf{45.1820 $\pm$ 1.9846} & 45.2080 $\pm$ 1.9012 & 46.3180 $\pm$ 2.3319\\
8 & 47.3560 $\pm$ 2.0675 & 31.8680 $\pm$ 0.3763 & \textbf{31.5920 $\pm$ 0.2770}\\
9 & 41.8000 $\pm$ 1.9664 & 20.8040 $\pm$ 0.2445 & \textbf{20.6780 $\pm$ 0.1299}\\
10 & 47.2940 $\pm$ 2.0270 & 43.1020 $\pm$ 1.7316 & \textbf{41.9280 $\pm$ 0.5276}\\
11 & 40.0120 $\pm$ 1.7352 & 26.7260 $\pm$ 0.2766 & \textbf{26.7120 $\pm$ 0.1591}\\
12 & 47.7680 $\pm$ 2.2787 & \textbf{47.5620 $\pm$ 2.2548} & 48.8500 $\pm$ 2.2433\\
13 & 41.9080 $\pm$ 1.6537 & \textbf{27.1000 $\pm$ 0.2470} & 27.1340 $\pm$ 0.1730\\
14 & 38.4440 $\pm$ 1.8358 & 26.7880 $\pm$ 0.2571 & \textbf{26.7540 $\pm$ 0.2027}\\
15 & 44.1560 $\pm$ 1.9865 & \textbf{25.3060 $\pm$ 0.2264} & 25.3200 $\pm$ 0.1729\\
16 & 47.3700 $\pm$ 2.3904 & 41.3460 $\pm$ 1.5358 & \textbf{40.5680 $\pm$ 0.4087}\\
17 & 43.4260 $\pm$ 1.7323 & 29.0940 $\pm$ 0.3272 & \textbf{28.9240 $\pm$ 0.1747}\\
18 & 40.9240 $\pm$ 1.5931 & 25.1700 $\pm$ 0.2385 & \textbf{25.1140 $\pm$ 0.1954}\\
19 & 40.5360 $\pm$ 1.6404 & 30.1420 $\pm$ 0.4425 & \textbf{29.9840 $\pm$ 0.2881}\\
20 & 43.5760 $\pm$ 1.9861 & \textbf{43.2420 $\pm$ 1.7934} & 44.7280 $\pm$ 2.2167\\
21 & 43.3680 $\pm$ 1.8529 & \textbf{42.7500 $\pm$ 1.7351} & 44.3600 $\pm$ 1.7702\\
22 & 40.8480 $\pm$ 1.6315 & 36.9640 $\pm$ 2.0682 & \textbf{36.6520 $\pm$ 0.3070}\\
23 & 38.1040 $\pm$ 2.0535 & \textbf{36.8040 $\pm$ 1.3680} & 38.7380 $\pm$ 1.6810\\
%\hline\hline
%\end{tabular}
%\end{table}
%
%\begin{table}[!htb]
%\tiny
%\begin{tabular}{|c||c|c|c|}
%\hline\hline
%Task &	PA                            & OHT1                          & OHT2 \\
%\hline\hline
%\input{err_table_tex}
24 & 40.0740 $\pm$ 1.7843 & \textbf{27.0860 $\pm$ 0.2640} & 27.1220 $\pm$ 0.1528\\
25 & 37.7980 $\pm$ 1.7306 & \textbf{23.8620 $\pm$ 0.2044} & 23.8840 $\pm$ 0.1308\\
26 & \textbf{45.3960 $\pm$ 1.8828} & 48.0940 $\pm$ 0.6795 & 46.6660 $\pm$ 2.0584\\
27 & 44.0160 $\pm$ 1.7706 & \textbf{31.7160 $\pm$ 0.3401} & 31.8080 $\pm$ 0.3203\\
28 & 34.2800 $\pm$ 1.6704 & 17.9340 $\pm$ 0.2413 & \textbf{17.8720 $\pm$ 0.1379}\\
29 & 47.6480 $\pm$ 2.3180 & \textbf{45.4580 $\pm$ 0.5772} & 46.6200 $\pm$ 1.2092\\
30 & 35.2360 $\pm$ 1.6471 & 19.9200 $\pm$ 0.2108 & \textbf{19.8540 $\pm$ 0.1359}\\
31 & 48.8520 $\pm$ 2.3983 & \textbf{40.5660 $\pm$ 0.5416} & 41.0820 $\pm$ 0.5235\\
32 & 42.9840 $\pm$ 2.1348 & 24.9140 $\pm$ 0.2400 & \textbf{24.8640 $\pm$ 0.1202}\\
33 & 42.8700 $\pm$ 1.7923 & \textbf{41.4680 $\pm$ 1.6964} & 43.3180 $\pm$ 1.1824\\
34 & 43.9460 $\pm$ 2.0912 & \textbf{34.3320 $\pm$ 0.5644} & 34.4200 $\pm$ 0.2719\\
35 & 38.5280 $\pm$ 1.6079 & \textbf{32.7740 $\pm$ 1.6567} & 32.7980 $\pm$ 0.2060\\
36 & 38.7800 $\pm$ 1.8058 & \textbf{26.4900 $\pm$ 0.3183} & 26.5300 $\pm$ 0.1667\\
37 & 44.5600 $\pm$ 1.8790 & 32.1100 $\pm$ 0.3380 & \textbf{32.0020 $\pm$ 0.2486}\\
38 & 39.8800 $\pm$ 1.6143 & \textbf{33.7560 $\pm$ 1.1056} & 34.4460 $\pm$ 0.2966\\
39 & 40.5700 $\pm$ 1.5981 & 35.5400 $\pm$ 1.5259 & \textbf{35.1480 $\pm$ 0.4279}\\
40 & 47.8280 $\pm$ 1.9353 & \textbf{46.3060 $\pm$ 0.5261} & 47.2580 $\pm$ 0.9894\\
41 & 41.8340 $\pm$ 2.0317 & 37.3080 $\pm$ 2.3227 & \textbf{36.3240 $\pm$ 0.3491}\\
42 & 36.7060 $\pm$ 1.3907 & \textbf{32.5580 $\pm$ 0.2189} & 32.6100 $\pm$ 0.2946\\
43 & 42.0780 $\pm$ 1.6208 & 34.3080 $\pm$ 1.3717 & \textbf{33.6300 $\pm$ 0.2890}\\
44 & 40.6600 $\pm$ 1.5188 & \textbf{33.7740 $\pm$ 0.7395} & 34.2240 $\pm$ 0.2731\\
45 & 42.1760 $\pm$ 2.0556 & 29.8760 $\pm$ 0.3514 & \textbf{29.7740 $\pm$ 0.2721}\\
\hline\hline
\end{tabular}
\label{results}
\end{table}

Table summarizes the mistake rates of all 45 tasks.
We see that in most tasks, PA has the very high mistake rate, which prove the dificulty of image classification task without any auxiliary source information.
The observation that our proposed OHT algorithms generally outperform PA validates the effectivity of heterogeneous transfer.

Figure illustrates the dynamic process of several online learning tasks, respectively.
We observe that in some tasks(e.g., 35, 39 and 41), the mistake rates of all three algorithms decrease during the period, and OHT algorithms always achieve better performance than PA.
Furthermore, in some tasks(e.g., 8, 11 and 24), OHT algorithms are able to obtain a good performance at the beginning stage and remain stable in the future.
These observations verifies that the OHT algorithms indeed transfer useful knowledge from the heterogeneous source domain to the target domain. 

\begin{figure*}[!htb]
\begin{center}
  %\subfigure[Task 8]
  %{
    %\label{fig:subfig:a}
    %\includegraphics[width=5cm]{task8.eps}
  %}
  \subfigure[Task 35]
  {
    \label{fig:subfig:b}
    \includegraphics[width=5cm]{task35.eps}
  }
  \subfigure[Task 39]
  {
    \label{fig:subfig:c}
    \includegraphics[width=5cm]{task39.eps}
  }
  \subfigure[Task 43]
  {
    \label{fig:subfig:d}
    \includegraphics[width=5cm]{task43.eps}
  }
  \\
  %\subfigure[Task 17]
  %{
    %\label{fig:subfig:e}
    %\includegraphics[width=5cm]{task17.eps}
  %}
  \subfigure[Task 18]
  {
    \label{fig:subfig:f}
    \includegraphics[width=5cm]{task18.eps}
  }
  \subfigure[Task 30]
  {
    \label{fig:subfig:g}
    \includegraphics[width=5cm]{task30.eps}
  }
  \subfigure[Task 36]
  {
    \label{fig:subfig:h}
    \includegraphics[width=5cm]{task36.eps}
  }
  \caption{Online mistake rates}
  \label{Online mistake rates}
\end{center}
\end{figure*}

We also analyze the performance difference between PA and two OHT algorithms.
Statistical significance against PA was assessed by paired $t$-test at 0.05 level.
For each task, a win (or loss) is counted when OHT algorithm is significatly better (or worse) than PA algorithm over 100 trials.
Otherwise, a tie is recorded.
The win/tie/loss results is 30/1/1 for competition between OHT1 and PA, and 30/1/1 for competition between  OHT2 and PA.
This result validates that our OHT algorithms is statistically better than PA algorithm.

Besides, we make use of Cohen's $d$ value to measure the improvement of our algorithms.
$d >= 0.8$ generally indicates a large promotion.
OHT1 algorithm achieves large improvement in 30 tasks and middle improvement in 3 tasks.
For OHT2 algorithms, the numbers are 32 and 2.

\subsection{Parameters and Running time}
% parameter sensitivity
\paragraph{Parameters}
Experiments in paper about online transfer learning illustrated that the performance of online transfer learning algorithms is generally insensitive to the parameter C and $\beta$.
Therefor, we only investigate how different values of parameter K affect the classification accuracy of the algorithms.
We select a number of tasks randomly to evaluate the parameter sensitivity.
Table shows the performance of the proposed algorithms with varied values of parameter K in task 1.
We observe that the performance of the algorithms is stable.
Similar obvervation are showed in other tasks.
In consideration of that we employ the weighted K nearest neighbors strategy, it is easy to understand.
The larger distance a instance in the heterogeneous source domain has, the less impact the instance makes.


% running time
\paragraph{Running time}
Table shows the mean and standard deviation of running time of different algorithms in several randomly selected tasks.
All of the algorithms were implemented in Matlab, and all experiments were run in a Linux machine with 3.2 GHz CPU and 3.8 GB memory.
From the table, we can see that PA without exploiting knowledge from the source domain is the most efficient.
Two OHT algorithms are less efficient.
The main reason of more running time is the searching process for the nearest neighbors.
Because of the insensitivity of parameter $K$, we can simply make use of all instances in the heterogeneous source domain to decrease the running time and obtain comparable performance.



\end{document}
